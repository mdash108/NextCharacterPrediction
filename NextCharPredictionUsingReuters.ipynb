{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a99d1ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "372/372 [==============================] - 68s 154ms/step - loss: 2.7565 - acc: 0.2048 - val_loss: 2.4701 - val_acc: 0.2771\n",
      "Epoch 2/10\n",
      "372/372 [==============================] - 60s 161ms/step - loss: 2.1348 - acc: 0.3562 - val_loss: 2.1137 - val_acc: 0.3520\n",
      "Epoch 5/10\n",
      "372/372 [==============================] - 57s 154ms/step - loss: 2.0287 - acc: 0.3850 - val_loss: 2.0429 - val_acc: 0.3687\n",
      "Epoch 6/10\n",
      "372/372 [==============================] - 55s 148ms/step - loss: 1.9312 - acc: 0.4181 - val_loss: 1.9808 - val_acc: 0.4103\n",
      "Epoch 7/10\n",
      "372/372 [==============================] - 52s 139ms/step - loss: 1.7554 - acc: 0.4724 - val_loss: 1.8853 - val_acc: 0.4368\n",
      "Epoch 9/10\n",
      "372/372 [==============================] - 57s 153ms/step - loss: 1.6797 - acc: 0.5014 - val_loss: 1.8497 - val_acc: 0.4504\n",
      "Epoch 10/10\n",
      "372/372 [==============================] - 54s 145ms/step - loss: 1.6035 - acc: 0.5201 - val_loss: 1.8139 - val_acc: 0.4693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x162e7bb8a30>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the reuters data\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# overall algo:\n",
    "# create a free flowing text without any punctuation etc. only alphabet\n",
    "# reuters data consists of sentences; each sentence is a list of words\n",
    "# so first of all combine all the words into one text string; then apply cleaning (only alphabet, \n",
    "# lower case, ' u  s ' to 'usa')\n",
    "\n",
    "# ok step 1\n",
    "# combine all words of all sentences \n",
    "sentLimit = 1000\n",
    "sentCnt = 0\n",
    "allText = ''\n",
    "for sentence in reuters.sents():\n",
    "    sentCnt += 1\n",
    "    if sentCnt <= sentLimit:\n",
    "        for word in sentence:\n",
    "            allText += word + ' '\n",
    "    else:\n",
    "        break\n",
    "# replace \" ' s '\" by \" \"\n",
    "import re\n",
    "allText = re.sub(\" ' s \", \" \", allText)\n",
    "allText = re.sub(r'([^a-zA-Z])', \" \", allText)\n",
    "allText = allText.lower()\n",
    "allText = re.sub(\" u   s \", \" usa \", allText)\n",
    "allText = re.sub(\"  \", \" \", allText)\n",
    "allText = \" \".join(allText.split())\n",
    "\n",
    "# now create sequences of seqLen (=5 say) characters where first four will be used for input and 5th char for output\n",
    "allSeq = []\n",
    "seqLen = 30\n",
    "for i in range(0, len(allText)-seqLen+1):\n",
    "    allSeq.append(allText[i:i+seqLen])\n",
    "\n",
    "# create a mapping of each character to an integer\n",
    "map1 = sorted(list(set(allText)))\n",
    "mapping = dict((c, i) for i, c in enumerate(map1))\n",
    "\n",
    "# now convert allSeq using this mapping\n",
    "allIntSeq = []\n",
    "for i in range(len(allSeq)):\n",
    "    allIntSeq.append([mapping[ch1] for ch1 in allSeq[i]])\n",
    "\n",
    "# using numpy create X and y from allIntSeq\n",
    "import numpy as np\n",
    "X, y = np.array(allIntSeq)[:, :-1], np.array(allIntSeq)[:, -1]\n",
    "\n",
    "# one-hot encode y to vector of lengthlen(map1)\n",
    "from keras.utils import to_categorical\n",
    "y = to_categorical(y, len(map1))\n",
    "\n",
    "# split X and y into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# now build the LSTM model: first import the libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(map1), 50, input_length=29, trainable=True))\n",
    "model.add(LSTM(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(len(map1), 'softmax'))\n",
    "#print(model.summary)\n",
    "model.compile(loss='categorical_crossentropy', metrics='acc', optimizer='adam')\n",
    "model.fit(X_tr, y_tr, epochs=10, verbose=1, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af174ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asian exporters fear damage from usa japan rift mounting trade friction between the usa and japan has raised fears among many of asia exporting nations that the row could inflict far reaching economic damage businessmen and officials said'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07732679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "but some the said\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# make prediction using seed_text\n",
    "seed_text=\"but som\"\n",
    "numPred = 10\n",
    "\n",
    "# make numPred predictions\n",
    "for i in range(numPred):\n",
    "    # convert to integers using mapping\n",
    "    seedSeq = [mapping[ch1] for ch1 in seed_text]\n",
    "    paddedSeq = pad_sequences([seedSeq], seqLen-1, truncating='pre')\n",
    "    yhat_probs = model.predict(paddedSeq)\n",
    "    for ch1, ind in mapping.items():\n",
    "        if ind == np.argmax(yhat_probs):\n",
    "            yhat = ch1\n",
    "    seed_text += yhat\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d1e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
